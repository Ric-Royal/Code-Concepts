{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anomaly detection\n",
    "#### Loading the csv and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "ctr = pd.read_csv(r'C:\\Users\\Rick-Royal\\Documents\\PROJECT\\ctr_eda1.csv')\n",
    "ctr.head()\n",
    "ctr.columns\n",
    "# from ctr, check for row entries containing punj in the organizationname column\n",
    "ctr.loc[ctr['OrganizationName'].str.contains('punj', case=False)]\n",
    "ctr.info()\n",
    "ctr.nunique()\n",
    "ctr.columns\n",
    "# include the figure size\n",
    "plt.figure(figsize=(10,10))\n",
    "# plot the correlation matrix\n",
    "corr = ctr.corr()\n",
    "sns.heatmap(corr,\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)\n",
    "\n",
    "\n",
    "corr\n",
    "# create a new dataframe without unique identifiers\n",
    "ctr = ctr.drop(['ReportDate', 'ReportNo', '_Transaction_day_of_week',\n",
    "       'ItemNo', 'FullName', 'AccountName', 'IDTypeAndNo','currency', 'TransactionDate',\n",
    "       'TransDescription', 'Report_year'], axis=1)\n",
    "# from ctr, label encode the following columns ,\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "ctr['Report_day_of_week'] = le.fit_transform(ctr['Report_day_of_week'])\n",
    "# ctr['_Transaction_day_of_week'] = le.fit_transform(ctr['_Transaction_day_of_week'])\n",
    "ctr['Industry'] = le.fit_transform(ctr['Industry'])\n",
    "ctr['OrganizationName'] = le.fit_transform(ctr['OrganizationName'])\n",
    "# One-hot encode the specified columns\n",
    "ctr_encoded = pd.get_dummies(ctr, columns=[ 'CurrencyType', 'person_type', 'Account_type']) # \n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(ctr_encoded.head())\n",
    "ctr_encoded.columns\n",
    "ctr_encoded.dtypes\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# # Load your dataframe (replace this with your actual dataframe)\n",
    "# # ctr = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# # Create instances of StandardScaler and MinMaxScaler\n",
    "# standard_scaler = StandardScaler()\n",
    "# minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# # Select the columns you want to scale (numerical columns)\n",
    "# columns_to_scale = ['col1', 'col2', 'col3']  # Replace with your actual column names\n",
    "\n",
    "# # Apply StandardScaler\n",
    "# ctr_standard_scaled = ctr.copy()\n",
    "# ctr_standard_scaled[columns_to_scale] = standard_scaler.fit_transform(ctr[columns_to_scale])\n",
    "\n",
    "# # Apply MinMaxScaler\n",
    "# ctr_minmax_scaled = ctr.copy()\n",
    "# ctr_minmax_scaled[columns_to_scale] = minmax_scaler.fit_transform(ctr[columns_to_scale])\n",
    "\n",
    "# # Check the resulting dataframes\n",
    "# print(\"Standard Scaled Data:\")\n",
    "# print(ctr_standard_scaled.head())\n",
    "\n",
    "# print(\"MinMax Scaled Data:\")\n",
    "# print(ctr_minmax_scaled.head())\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create instances of StandardScaler and MinMaxScaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Select the columns you want to scale (numerical columns)\n",
    "columns_to_scale = ['Amount', 'days_diff', 'Transaction_year', 'Transaction_month',\n",
    "       'Transaction_day', 'Transaction_day_of_week',\n",
    "       'Report_month', 'Report_day', 'Report_day_of_week', 'Amount_KES']  # Replace with your actual column names\n",
    "\n",
    "# Apply StandardScaler\n",
    "ctr_standard_scaled = ctr_encoded.copy()\n",
    "ctr_standard_scaled[columns_to_scale] = standard_scaler.fit_transform(ctr_encoded[columns_to_scale])\n",
    "\n",
    "# Check the resulting dataframes\n",
    "print(\"Standard Scaled Data:\")\n",
    "print(ctr_standard_scaled.head())\n",
    "\n",
    "\n",
    "\n",
    "ctr_standard_scaled.head()\n",
    "# from ctr_standard_scaled, split the data into train and test for an unlabeled dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(ctr_standard_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "### Anomaly detection\n",
    "# Import libraries for isolation forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import silhouette_score, calinski_harabasz_score, davies_bouldin_score, kolmogorov_smirnov_test\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Create an isolation forest model\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(X_train)\n",
    "\n",
    "# Predict the model\n",
    "y_pred = isolation_forest.predict(X_test)\n",
    "\n",
    "\n",
    "# Importing Libraries\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.utils.data import generate_data\n",
    "\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "\n",
    "anomaly_proportion = 0.001\n",
    "\n",
    "# Training IForest indicator\n",
    "clf_name = 'Anomaly detection - Isolation forest'\n",
    "clf = IForest(contamination = anomaly_proportion)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train)\n",
    "\n",
    "\n",
    "X_train['y_pred'] = clf.labels_ #binary labels (0=inlier, 1=outlier)\n",
    "X_train['y_scores'] = clf.decision_scores_ # raw outlier scores. The bigger the number the greater the anomaly.\n",
    "xx , yy = np.meshgrid(np.linspace(0, 11, 200), np.linspace(0, 180000, 200))\n",
    "\n",
    "# decision function calculates the raw anomaly score for every point\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])*-1\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "\n",
    "threshold = (X_train.loc[X_train['y_pred'] == 1, 'y_scores'].min()*-1)/2 + (X_train.loc[X_train['y_pred'] == 0, 'y_scores'].max()*-1)/2\n",
    "\n",
    "\n",
    "subplot = plt.subplot(1, 1, 1)\n",
    "\n",
    "# fill blue colormap from minimum anomaly score to threshold value\n",
    "subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(), threshold, 10),cmap=plt.cm.Blues_r)\n",
    "\n",
    "# draw red contour line where anomaly score is equal to threshold\n",
    "a = subplot.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n",
    "\n",
    "# fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n",
    "subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n",
    "\n",
    "msk = X_train['y_pred'] == 0\n",
    "x = X_train.loc[msk, ['Amount', 'days_diff', 'Transaction_year', 'Transaction_month',\n",
    "       'Transaction_day', 'Transaction_day_of_week', 'Report_year',\n",
    "       'Report_month', 'Report_day', 'Report_day_of_week', 'Amount_KES',\n",
    "       'person_type_entity', 'person_type_human', 'Account_type_business',\n",
    "       'Account_type_personal', 'currency_AUD', 'currency_CAD', 'currency_CHF',\n",
    "       'currency_DKK', 'currency_EGP', 'currency_EUR', 'currency_GBP',\n",
    "       'currency_HKK', 'currency_INR', 'currency_JPY', 'currency_KES',\n",
    "       'currency_KOW', 'currency_NPR', 'currency_PAR', 'currency_PKR',\n",
    "       'currency_QTR', 'currency_RUB', 'currency_SDL', 'currency_SEK',\n",
    "       'currency_SFR', 'currency_UAE', 'currency_USD', 'currency_ZAR']].values\n",
    "\n",
    "# scatter plot of inliers with white dots\n",
    "b = subplot.scatter(x[:, 0], x[:, 1], c='white',s=20, edgecolor='k') \n",
    "\n",
    "\n",
    "msk = X_train['y_pred'] == 1\n",
    "x = X_train.loc[msk, ['Amount', 'days_diff', 'Transaction_year', 'Transaction_month',\n",
    "       'Transaction_day', 'Transaction_day_of_week', 'Report_year',\n",
    "       'Report_month', 'Report_day', 'Report_day_of_week', 'Amount_KES',\n",
    "       'person_type_entity', 'person_type_human', 'Account_type_business',\n",
    "       'Account_type_personal', 'currency_AUD', 'currency_CAD', 'currency_CHF',\n",
    "       'currency_DKK', 'currency_EGP', 'currency_EUR', 'currency_GBP',\n",
    "       'currency_HKK', 'currency_INR', 'currency_JPY', 'currency_KES',\n",
    "       'currency_KOW', 'currency_NPR', 'currency_PAR', 'currency_PKR',\n",
    "       'currency_QTR', 'currency_RUB', 'currency_SDL', 'currency_SEK',\n",
    "       'currency_SFR', 'currency_UAE', 'currency_USD', 'currency_ZAR']].values\n",
    "\n",
    "# scatter plot of outliers with black dots\n",
    "c = subplot.scatter(x[:, 0], x[:, 1], c='black',s=20, edgecolor='r')\n",
    "subplot.axis('tight')\n",
    "\n",
    "subplot.legend(\n",
    "    [a.collections[0], b, c],\n",
    "    ['learned decision function', 'inliers', 'outliers'],\n",
    "    prop=plt.font_manager.FontProperties(size=10),\n",
    "    loc='upper right')\n",
    "subplot.set_title(clf_name)\n",
    "subplot.set_xlim((0, 11))\n",
    "subplot.set_ylim((0, 180000))\n",
    "\n",
    "subplot.set_xlabel(\"5-day count of withdrawal transactions.\")\n",
    "subplot.set_ylabel(\"5-day sum of withdrawal transactions\")\n",
    "# from ctr, check for row entries containing punj in the organizationname column\n",
    "ctr.loc[ctr['OrganizationName'].str.contains('punj', case=False)]\n",
    "\n",
    "### PCA Decomposition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming ctr_encoded is a pandas DataFrame\n",
    "# Standardize the features for better PCA performance\n",
    "scaler = StandardScaler()\n",
    "ctr_standard_scaled = scaler.fit_transform(ctr_encoded)\n",
    "\n",
    "# Initialize PCA and fit it to the standardized data\n",
    "pca = PCA()\n",
    "pca.fit(ctr_standard_scaled)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the efficient number of components to explain more than 99% of the variance\n",
    "n_components = np.where(cumulative_explained_variance >= 0.99)[0][0] + 1\n",
    "\n",
    "# Perform PCA with the efficient number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "ctr_pca = pca.fit_transform(ctr_standard_scaled)\n",
    "\n",
    "# Use k-means clustering on the reduced dataset\n",
    "kmeans = KMeans(n_clusters=3)  # Choose the desired number of clusters (k)\n",
    "kmeans.fit(ctr_pca)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming ctr_pca is a 2D numpy array with the PCA-transformed data\n",
    "# and clusters is a 1D numpy array with the cluster assignments\n",
    "\n",
    "# Create a scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(ctr_pca[:, 0], ctr_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Add a colorbar with cluster labels\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks([0, 1, 2])  # Change this to the number of clusters - 1\n",
    "colorbar.set_ticklabels(['Cluster 0', 'Cluster 1', 'Cluster 2'])  # Change this to the number of clusters\n",
    "\n",
    "# Label the axes with the principal components\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Set the title and display the plot\n",
    "plt.title('K-means Clustering with PCA-Reduced Data')\n",
    "plt.show()\n",
    "\n",
    "# 1. Cluster 1- Medium risk,\n",
    "# 2. Cluster 2- low risk,\n",
    "# 3. Cluster 0- high risk,\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "if ctr_pca.shape[1] > 1:\n",
    "    scatter = plt.scatter(ctr_pca[:, 0], ctr_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "else:\n",
    "    scatter = plt.scatter(ctr_pca[:, 0], np.zeros_like(ctr_pca[:, 0]), c=clusters, cmap='viridis', alpha=0.5)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.colorbar(scatter, label='Cluster', ticks=range(kmeans.n_clusters))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Assuming ctr is the original dataset as a pandas DataFrame\n",
    "# and clusters is a 1D numpy array with the cluster assignments\n",
    "\n",
    "# Add the cluster assignments as a new column to the original dataset\n",
    "ctr['cluster'] = clusters\n",
    "\n",
    "# Print the merged dataset with cluster assignments\n",
    "print(ctr.head())\n",
    "\n",
    "# plot a scatter plot of the 2 features and color the points by cluster\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(ctr['days_diff'], ctr['Amount_KES'], c=ctr['cluster'], cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Add a colorbar with cluster labels\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks([0, 1, 2])  # Change this to the number of clusters - 1\n",
    "colorbar.set_ticklabels(['Cluster 0', 'Cluster 1', 'Cluster 2'])  # Change this to the number of clusters\n",
    "\n",
    "# Label the axes    \n",
    "plt.xlabel('days_diff')\n",
    "plt.ylabel('Amount_KES')\n",
    "\n",
    "# Set the title and display the plot\n",
    "plt.title('K-means Clustering with PCA-Reduced Data')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot a bar plot of the cluster labels and AMount_KES\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(x=ctr['cluster'], y=ctr['Amount_KES'])\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Amount_KES')\n",
    "\n",
    "# Set the title and display the plot    \n",
    "plt.title('Average Amount_KES per Cluster')\n",
    "plt.show()\n",
    "\n",
    "ctr_1 = pd.read_csv(r'C:\\Users\\Rick-Royal\\Documents\\PROJECT\\ctr_eda1.csv')\n",
    "\n",
    "# Add the cluster assignments as a new column to the original dataset\n",
    "ctr_1['cluster'] = clusters\n",
    "\n",
    "# Print the merged dataset with cluster assignments\n",
    "print(ctr_1.columns)\n",
    "# ctr_1.to_csv('ctr_cluster_t2.csv', index=False)\n",
    "# # from ctr_encoded, conduct PCA by finding the efficient number of components\n",
    "# # select the number of components such that the amount of variance that needs to be explained is greater than 95%\n",
    "# # the PCA will be used to reduce the number of features in the dataset\n",
    "# # the PCA dataset will be used in finding clusters in the dataset using k-means clustering\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=0.95)\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# # the number of components that explain 95% of the variance in the dataset\n",
    "# pca.n_components_\n",
    "\n",
    "# # the variance explained by each of the components\n",
    "# pca.explained_variance_ratio_\n",
    "\n",
    "# # the cumulative variance explained by the components\n",
    "# pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# # the components\n",
    "# pca.components_\n",
    "\n",
    "# # the transformed dataset\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "\n",
    "# # the transformed dataset\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "#### EDA\n",
    "# from ctr_1, plot a scatter plot of amount_kes and days_diff and color the points by cluster\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(ctr_1['days_diff'], ctr_1['Amount_KES'], c=ctr_1['cluster'], cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Add a colorbar with cluster labels\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks([0, 1, 2])  # Change this to the number of clusters - 1\n",
    "colorbar.set_ticklabels(['Cluster 0', 'Cluster 1', 'Cluster 2'])  # Change this to the number of clusters\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('days_diff')\n",
    "plt.ylabel('Amount_KES')\n",
    "\n",
    "# Set the title and display the plot\n",
    "plt.title('Amount_KES vs days_diff')\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the plot style and size\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Create a bar plot of the number of transactions per cluster\n",
    "ax = sns.countplot(x='cluster', data=ctr)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Number of Transactions')\n",
    "ax.set_title('Number of Transactions per Cluster')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the plot style and size\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create a histogram of transaction amounts for each cluster\n",
    "ax = sns.histplot(data=ctr, x='Amount_KES', hue='cluster', bins=50, kde=True, alpha=0.6, multiple='stack')\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Transaction Amount (KES)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Transaction Amounts by Cluster')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the plot style and size\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create a box plot of transaction amounts for each cluster\n",
    "ax = sns.boxplot(x='cluster', y='Amount_KES', data=ctr)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Transaction Amount (KES)')\n",
    "ax.set_title('Distribution of Transaction Amounts by Cluster')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure for the days_diff box plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create a box plot of days_diff for each cluster\n",
    "ax = sns.boxplot(x='cluster', y='days_diff', data=ctr)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Days Difference')\n",
    "ax.set_title('Distribution of Days Difference by Cluster')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the average transaction amount for each industry and cluster combination\n",
    "average_amounts = ctr_1.groupby(['Industry', 'cluster'])['Amount_KES'].mean().unstack()\n",
    "\n",
    "# Set the plot style and size\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create a heatmap of the average transaction amounts\n",
    "ax = sns.heatmap(average_amounts, annot=True, fmt='.2f', cmap='YlGnBu', linewidths=0.5)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Industry')\n",
    "ax.set_title('Average Transaction Amount by Industry and Cluster')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# In this code, we first calculate the average transaction amount (Amount_KES) for each industry and cluster combination using groupby() and mean() functions. Then, we create a heatmap using sns.heatmap. The annot parameter is set to True, which displays the average transaction amount in each cell. The fmt parameter is set to '.2f', which formats the numbers to two decimal places. The cmap parameter is set to 'YlGnBu', which controls the colormap used for the heatmap. The linewidths parameter is set to 0.5, which adds a thin border between cells.\n",
    "\n",
    "# Adjust the parameters as needed to customize the appearance of the heatmap.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the silhouette score\n",
    "sil_score = silhouette_score(ctr_pca, clusters)\n",
    "\n",
    "print(\"Silhouette score:\", sil_score)\n",
    "\n",
    "ctr_1.columns\n",
    "ctr_1.info()\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert TransactionDate to datetime and extract the month and year\n",
    "ctr_1['TransactionDate'] = pd.to_datetime(ctr_1['TransactionDate'])\n",
    "ctr_1['Transaction_month_year'] = ctr_1['TransactionDate'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Calculate the total transaction amount and the number of transactions per month for each cluster\n",
    "monthly_amounts = ctr_1.groupby(['Transaction_month_year', 'cluster'])['Amount_KES'].sum().reset_index()\n",
    "monthly_transactions = ctr_1.groupby(['Transaction_month_year', 'cluster']).size().reset_index(name='transaction_count')\n",
    "\n",
    "# Set the plot style and size\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create a line plot of the total transaction amount per month for each cluster\n",
    "ax = sns.lineplot(x='Transaction_month_year', y='Amount_KES', hue='cluster', data=monthly_amounts)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Month/Year')\n",
    "ax.set_ylabel('Total Transaction Amount (KES)')\n",
    "ax.set_title('Total Transaction Amount by Month and Cluster')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure for the number of transactions plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create a line plot of the number of transactions per month for each cluster\n",
    "ax = sns.lineplot(x='Transaction_month_year', y='transaction_count', hue='cluster', data=monthly_transactions)\n",
    "\n",
    "# Set the x and y-axis labels and plot title\n",
    "ax.set_xlabel('Month/Year')\n",
    "ax.set_ylabel('Number of Transactions')\n",
    "ax.set_title('Number of Transactions by Month and Cluster')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# In this code, we first convert the TransactionDate column to a datetime object and extract the month and year into a new column called Transaction_month_year. Then, we calculate the total transaction amount and the number of transactions per month for each cluster using groupby() and sum() or size() functions. After that, we create a line plot for both total transaction amount and the number of transactions using sns.lineplot. The x-axis represents the month and year, and the y-axis represents the total transaction amount or the number of transactions. The hue parameter is set to 'cluster', which colors the lines according to the cluster.\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.scatterplot(x='Transaction_month_year', y='Amount_KES', size='days_diff', hue='cluster', data=ctr_1, alpha=0.6, sizes=(20, 200))\n",
    "plt.xlabel('Transaction Month/Year')\n",
    "plt.ylabel('Transaction Amount (KES)')\n",
    "plt.title('Transaction Amounts by Month and Days Difference')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.violinplot(x='cluster', y='Amount_KES', data=ctr_1, inner='quartile')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Transaction Amount (KES)')\n",
    "plt.title('Distribution of Transaction Amounts by Cluster')\n",
    "plt.show()\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = ctr_1[['Amount_KES', 'days_diff', 'Transaction_year', 'Transaction_month', 'cluster']].corr()\n",
    "\n",
    "# Create a clustermap\n",
    "sns.clustermap(corr_matrix, annot=True, cmap='coolwarm', figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "g = sns.PairGrid(ctr_1[['Amount_KES', 'days_diff', 'Transaction_year', 'Transaction_month', 'cluster']], hue='cluster')\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n",
    "g.add_legend()\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(ctr_1[['Amount_KES', 'days_diff', 'Transaction_year', 'Transaction_month', 'cluster']], hue='cluster')\n",
    "plt.show()\n",
    "\n",
    "industry_cluster_count = ctr_1.groupby(['Industry', 'cluster']).size().unstack().fillna(0)\n",
    "\n",
    "# Normalize the data for better comparison\n",
    "normalized_industry_cluster_count = industry_cluster_count.div(industry_cluster_count.sum(axis=1), axis=0)\n",
    "\n",
    "normalized_industry_cluster_count.plot(kind='bar', stacked=True, figsize=(14, 7))\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Proportion of Clusters by Industry')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Select the columns you want to plot and normalize the numerical columns\n",
    "selected_columns = ctr_1[['Amount_KES', 'days_diff', 'Transaction_month', 'cluster']]\n",
    "normalized_selected_columns = (selected_columns - selected_columns.min()) / (selected_columns.max() - selected_columns.min())\n",
    "\n",
    "# Keep the 'cluster' column as integers\n",
    "normalized_selected_columns['cluster'] = selected_columns['cluster']\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "parallel_coordinates(normalized_selected_columns, 'cluster', colormap='viridis')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Normalized Values')\n",
    "plt.title('Parallel Coordinates Plot of Selected Variables by Cluster')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "g = sns.FacetGrid(ctr_1, col='cluster', hue='cluster', height=4, aspect=1)\n",
    "g.map(sns.scatterplot, 'days_diff', 'Amount_KES', alpha=0.7)\n",
    "g.add_legend()\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Days Difference vs. Transaction Amount by Cluster')\n",
    "plt.show()\n",
    "#### Kmeans Clustering\n",
    "# import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assume that the ctr_encoded dataframe is already created\n",
    "\n",
    "# Perform feature scaling (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "ctr_encoded_scaled = scaler.fit_transform(ctr_encoded)\n",
    "\n",
    "# Determine the optimal number of clusters (k) using the silhouette score\n",
    "max_clusters = 10  # Adjust this value based on your dataset size and requirements\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(ctr_encoded_scaled)\n",
    "    silhouette_avg = silhouette_score(ctr_encoded_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"Silhouette score for k={k}: {silhouette_avg}\")\n",
    "\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Fit the KMeans model using the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "ctr_encoded['cluster'] = kmeans.fit_predict(ctr_encoded_scaled)\n",
    "\n",
    "# Check the resulting dataframe with cluster labels\n",
    "print(ctr_encoded.head())\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Determine the optimal number of clusters (k) using the silhouette score\n",
    "max_clusters = 10  # Adjust this value based on your dataset size and requirements\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(ctr_standard_scaled)\n",
    "    silhouette_avg = silhouette_score(ctr_standard_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"Silhouette score for k={k}: {silhouette_avg}\")\n",
    "\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Fit the KMeans model using the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "ctr_encoded['cluster'] = kmeans.fit_predict(ctr_standard_scaled)\n",
    "\n",
    "# Check the resulting dataframe with cluster labels\n",
    "print(ctr_encoded.head())\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume the ctr_encoded dataframe has been clustered using KMeans\n",
    "# and the resulting cluster labels are stored in the 'cluster' column\n",
    "\n",
    "# Perform PCA to reduce the dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(ctr_encoded)\n",
    "\n",
    "# Create a new dataframe with principal components and cluster labels\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['principal_component_1', 'principal_component_2'])\n",
    "pca_df['cluster'] = ctr_encoded['cluster']\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in pca_df['cluster'].unique():\n",
    "    plt.scatter(pca_df.loc[pca_df['cluster'] == cluster, 'principal_component_1'],\n",
    "                pca_df.loc[pca_df['cluster'] == cluster, 'principal_component_2'],\n",
    "                label=f\"Cluster {cluster}\")\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.title('KMeans Clustering')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
